
from pyspark.sql import  SQLContext,SparkSession
#import microsoft_malware.bytes_ngram_processing as bgrams
import microsoft_malware.file_processor as bgrams;
import microsoft_malware.bytes_ngram_processing as m
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import VectorAssembler











class Train_Malware(object) :

    def __init__(self ,sc):
        self.spartContext=sc;
        print()

    def extractFeatures(self):
        X_train=self.spartContext.textFile('gs://uga-dsp/project2/files/X_train.txt') # gs://uga-dsp/project2/files/X_train.txt   --cloud url
        Y_train=self.spartContext.textFile('gs://uga-dsp/project2/files/y_train.txt') # gs://uga-dsp/project2/files/y_train.txt   -- cloud url
        bytes_text_file_rdd = X_train.map(lambda x: "gs://uga-dsp/project2/data/bytes/" + x + ".bytes")
        bytes_text_files = bytes_text_file_rdd.reduce(lambda x, y: x + "," + y); # comma seperated bytes file
        asm_text_file_rdd = X_train.map(lambda x: "gs://uga-dsp/project2/data/bytes/" + x + ".asm")
        X_train=X_train.zipWithIndex()
        X_train=X_train.map(lambda x:(x[1],x[0]))  #(docid,hash)
        Y_train=Y_train.zipWithIndex()
        Y_train=Y_train.map(lambda x:(x[1],int(x[0]))) #(docid,label)
        asm_text_files = asm_text_file_rdd.reduce(lambda x, y: x + "," + y); # comma seperated asm, files


        zippedRdd=X_train.join(Y_train)

        zippedRdd = zippedRdd.map(lambda x: (x[1][0], (int(x[1][1]), x[0]))) #(hash,(label,docid))
        file_processor1=fp.File_Processor1(True,True)
        file_rdd=file_processor1.process_Files(sc,zippedRdd)
        file_processor = fp.File_Processor(True, True)
        file_processor.process_Files(self.spartContext, zippedRdd,bytes_text_files,asm_text_files)
        bytesprocessor=bgrams.Bytes_Ngrams_processor(1, True,True)
        bytesprocessor.extractNgramFeatures(self.spartContext,zippedRdd,bytes_text_files)
        bytesprocessor = bgrams.Bytes_Ngrams_processor(2, True,True)
        bytesprocessor.extractNgramFeatures(self.spartContext, zippedRdd,bytes_text_files)








class Test_Malware(object):
    def __init__(self, sc):
        self.spartContext = sc;

    def extractFeatures(self):
        labelsFlag=False;
        if labelsFlag==True:
            X_test = self.spartContext.textFile('gs://uga-dsp/project2/files/X_small_test.txt')
            Y_test = self.spartContext.textFile('gs://uga-dsp/project2/files/Y_small_test.txt')
            X_test = X_test.zipWithIndex()
            Y_test = Y_test.zipWithIndex()
            X_test=X_test.map(lambda x:(x[1],x[0]))
            Y_test=Y_test.map(lambda x:(x[1],x[0]))

            zippedRdd = X_test.join(Y_test)
            zippedRdd = zippedRdd.map(lambda x: (x[1][0], (int(x[1][1]), x[0]))) #(hash,(label,docid))
            file_processor1 = fp.File_Processor1(False, True)
            file_rdd = file_processor1.process_Files(sc, zippedRdd)



            #file_processor = fp.File_Processor1(False, True)
            #file_processor.processFiles(self.spartContext, zippedRdd)
            #bytesprocessor = bgrams.Bytes_Ngrams_processor(1, False, labelsFlag)
            #bytesprocessor.extractNgramFeatures(self.spartContext, zippedRdd)
            #bytesprocessor = bgrams.Bytes_Ngrams_processor(2, False, labelsFlag)
            #bytesprocessor.extractNgramFeatures(self.spartContext, zippedRdd)

        else:
            X_test = self.spartContext.textFile('./files/X_train.txt')
            bytes_text_file_rdd = X_test.map(lambda x: "gs://uga-dsp/project2/data/bytes/" + x + ".bytes")
            bytes_text_files = bytes_text_file_rdd.reduce(lambda x, y: x + "," + y);  # comma seperated bytes file
            asm_text_file_rdd = X_test.map(lambda x: "gs://uga-dsp/project2/data/bytes/" + x + ".asm")
            asm_text_files = asm_text_file_rdd.reduce(lambda x, y: x + "," + y);  # comma seperated asm, files

            X_test = X_test.zipWithIndex()
            zippedRdd = X_test.map(lambda x: (x[0],x[1])) #(hash,docid)
            #file_processor1 = fp.File_Processor(False, False)
            #file_rdd = file_processor1.process_Files(sc, zippedRdd,asm_text_files,bytes_text_files)
            file_processor = fp.File_Processor1(False, False)
            file_processor.process_Files(self.spartContext, zippedRdd)
            #bytesprocessor = bgrams.Bytes_Ngrams_processor(1, False, labelsFlag)
            #bytesprocessor.extractNgramFeatures(self.spartContext, zippedRdd)
            #bytesprocessor = bgrams.Bytes_Ngrams_processor(2, False, labelsFlag)
            #bytesprocessor.extractNgramFeatures(self.spartContext, zippedRdd)



class CombineFeatures(object):
    def __init__(self, sc):
        self.sparkContext = sc;

    def extractFeatures(self):

        sql=SQLContext(self.sparkContext)
        train_File=sql.read.parquet('./data/train-file-features/train-file.parquet')
        train_File_1_grams=sql.read.parquet('./data/train-bytes/train-1gram.parquet')
        train_File_2_grams=sql.read.parquet('./data/train-bytes/train-2gram.parquet')

        #train_rdd=train_File.join(train_File_1_grams,(train_File.docid==train_File_1_grams.docid) & (train_File.hash==train_File_1_grams.hash))
        train_rdd=train_File.join(train_File_1_grams,["docid","hash","label"])
        train_rdd=train_rdd.join(train_File_2_grams,["docid","hash","label"])
        test_File=sql.read.parquet('./data/test-file-features/test-file.parquet')
        test_File_1_grams = sql.read.parquet('./data/test-bytes/test-1gram.parquet')
        test_File_2_grams = sql.read.parquet('./data/test-bytes/test-2gram.parquet')
        ylabel=True;

        if ylabel==True :
            test_rdd = test_File.join(test_File_1_grams, ["docid", "hash", "label"]).join(test_File_2_grams,["docid", "hash", "label"])
        else:
            test_rdd = test_File.join(test_File_1_grams, ["docid", "hash" ]).join(test_File_2_grams,["docid", "hash"])


        print(test_rdd.show())
        assembler = VectorAssembler().setInputCols(["bytes_size", "asm_size", "asm_byte_ratio","1grams","2grams"]).setOutputCol(
            "features")
        training_file_Df = assembler.transform(train_rdd)
        test_file_Df = assembler.transform(test_rdd)

        rf = RandomForestClassifier(numTrees=200, impurity='gini', maxDepth=8, maxBins=32, seed=42)
        model = rf.fit(training_file_Df)

        # Make predictions.
        result = model.transform(test_file_Df)
        # save to csv


        result.select('docid', 'hash', 'prediction').toPandas().to_csv('largesetpredictions.csv', header=False,index=False)
        spark.stop()

if __name__=='__main__' :
    spark = SparkSession \
        .builder \
        .appName("Catherine") \
        .getOrCreate()

    sc = spark.sparkContext

    train_Malware=Train_Malware(sc)
    train_Malware.extractFeatures()
    test_Malware=Test_Malware(sc)
    test_Malware.extractFeatures()


    #common_features=CombineFeatures(sc);
    #common_features.extractFeatures()


