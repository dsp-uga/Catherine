# python packages
import argparse
import os.path
import re
import numpy as np
from operator import add

# pyspark packages
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.ml.feature import NGram
from pyspark.ml.linalg import Vectors

def segment_detect(asm_content):
    """
    Detects segment words in the whole content of asm files.
    Returns a list of segment words.
    """
    pattern = re.compile(r'([A-Za-z]+):[0-9A-Z]{8}[\s+]')
    pattern_list = pattern.findall(asm_content)
    return pattern_list

def numpy_cartesian(feature_list, n):
    """
    Return a numpy array containing 0s for not specified index,
    but feature counts for specified index.
    """
    feature_array = np.zeros((n,), dtype = np.int)
    if feature_list == None:
        feature_array = np.zeros((n,), dtype = np.int)
    else:
        f = np.asarray(feature_list)
        feature_array[f[:,0]] = f[:,1]
    return feature_array


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description = "CSCI 8360 Project 2",
        epilog = "answer key", add_help = "How to use",
        prog = "python segment_cnt.py [txt_files_path] [asm_files_path] [output_path]")

    # Required args
    parser.add_argument("file-path", help = "Directory of .txt files")
    parser.add_argument("asm-path", help = "Directory of .asm files")
    parser.add_argument("output-path", help = "Directory of outputs")
    # Optional args
    parser.add_argument("-s", "--size", choices = ["small", "large"], default = "small",
        help = "Size to the selected file: \"small\", \"large\" [Default: \"small\"]")

    args = vars(parser.parse_args())
    sc = SparkContext()
    spark = SparkSession.builder.master("local").appName("Word Count")\
                        .config("spark.some.config.option", "some-value").getOrCreate()

    file_path = str(args['file-path'])
    asm_path = str(args['asm-path'])
    output_path = str(args['output-path'])
    if args['size'] == "small": size = str('_small')
    if args['size'] == "large": size = str('')

    # .txt files
    print('***** Reading txt files *******************************************************************')
    rdd_Xtrain = sc.textFile(file_path + 'X'+size+'_train.txt').zipWithIndex().map(lambda x: (x[1], x[0]))
    rdd_ytrain = sc.textFile(file_path + 'y'+size+'_train.txt').zipWithIndex().map(lambda x: (x[1], x[0]))
    rdd_train = rdd_Xtrain.join(rdd_ytrain).sortByKey().map(lambda x: (x[0],) + x[1])
    rdd_Xtest = sc.textFile(file_path + 'X'+size+'_test.txt').zipWithIndex().map(lambda x: (x[1], x[0]))

    # .asm files
    print('***** Reading asm training files **********************************************************')
    files_train = rdd_Xtrain.map(lambda x: asm_path+x[1]+'.asm').reduce(lambda accum,x: accum+','+x)
    rdd_asm_train = sc.wholeTextFiles(files_train)\
                    .map(lambda x: (os.path.basename(x[0]), x[1])).map(lambda x: (x[0].replace('.asm', ''), x[1]))
    print('***** Reading asm testing files ***********************************************************')
    files_test = rdd_Xtest.map(lambda x: asm_path+x[1]+'.asm').reduce(lambda accum,x: accum+','+x)
    rdd_asm_test = sc.wholeTextFiles(files_test)\
                    .map(lambda x: (os.path.basename(x[0]), x[1])).map(lambda x: (x[0].replace('.asm', ''), x[1]))

    # Training set
    # -------------------------------------------------------------------------
    print('***** Training set starts *****************************************************************')
    print('***** Detecting segments ******************************************************************')
    # >> ((filename, segment), count)
    rdd_segment_cnt = rdd_asm_train.map(lambda x: (x[0], segment_detect(x[1])))\
                            .flatMapValues(lambda x: x).map(lambda x: (x,1)).reduceByKey(add)

    print('***** Creating distinct segments list *****************************************************')
    # >> (segment, index)
    rdd_segment_distinct = rdd_segment_cnt.map(lambda x: x[0][1]).distinct().sortBy(lambda x: x).zipWithIndex()
    N = rdd_segment_distinct.count()

    print('***** Creating segments list for each document ********************************************')
    # >> (hash, list((segment_index, cnt), (segment_index, cnt), ...))
    rdd_segment = rdd_segment_cnt.map(lambda x: (x[0][1], (x[0][0], x[1])))\
                    .leftOuterJoin(rdd_segment_distinct)\
                    .map(lambda x: (x[1][0][0], (x[1][1], x[1][0][1])))\
                    .groupByKey().map(lambda x: (x[0], list(x[1])))

    print('***** Creating segments list with document information ************************************')
    # >> (docid, hash, label, vector.dense(segment))
    segment = rdd_train.map(lambda x: (x[1], (x[0], x[2]))).leftOuterJoin(rdd_segment)\
                .map(lambda x: (x[1][0][0], x[0], x[1][0][1], list(numpy_cartesian(x[1][1], N))))\
                .map(lambda x: (x[0], x[1], x[2], Vectors.dense(x[3])))

    print('***** Transforming RDD into Dateframe *****************************************************')
    df_segment_train = spark.createDataFrame(segment)
    print('***** Outputing parquet file **************************************************************')
    df_segment_train.write.parquet(output_path + "segment" + size + "_train/")

    # Testing set
    # -------------------------------------------------------------------------
    print('***** Testing set starts ******************************************************************')
    print('***** Detecting segments ******************************************************************')
    # >> ((filename, segment), count)
    rdd_segment_cnt_test = rdd_asm_test.map(lambda x: (x[0], segment_detect(x[1])))\
                            .flatMapValues(lambda x: x).map(lambda x: (x,1)).reduceByKey(add)

    print('***** Creating segments list for each document ********************************************')
    # >> (hash, (segment_index, segment_cnt))
    rdd_segment_test = rdd_segment_distinct\
                            .leftOuterJoin(rdd_segment_cnt_test.map(lambda x: (x[0][1], (x[0][0], x[1]))))\
                            .filter(lambda x: x[1][1]!=None)\
                            .map(lambda x: (x[1][1][0], (x[1][0], x[1][1][1])))\
                            .groupByKey().map(lambda x: (x[0], list(x[1])))

    print('***** Creating segments list with document information ************************************')
    # >> (docid, hash, vector.dense(segment))
    segment_test = rdd_Xtest.map(lambda x: (x[1], x[0])).leftOuterJoin(rdd_segment_test)\
                        .map(lambda x: (x[0], x[1][0], list(numpy_cartesian(x[1][1], N))))\
                        .map(lambda x: (x[0], x[1], Vectors.dense(x[2])))

    print('***** Transforming RDD into Dateframe *****************************************************')
    df_segment_test = spark.createDataFrame(segment_test)
    print('***** Outputing parquet file **************************************************************')
    df_segment_test.write.parquet(output_path + "segment" + size + "_test/")
