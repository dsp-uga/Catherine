from pylab import *
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, NumericType, FloatType
import urllib.request as req_file
import ntpath
from sys import getsizeof



class File_Processor(object):
    '''
    This class takes in the bytes files and asm files size and the calculates the bytes size asm size and file size
    and ratio of bytes and asm size

    methods

     extractfileName :- Takes in the filename and returns hash value
     calculateFileSize :- calculates file sizes of asm and bytes file
     processFiles :- Takes in rdds of asm file and bytes file and calculates (bytes ,asm and asm/bytes ratio)



    '''
    def __init__(self,train=False,yLabel=False):
        self.train=train
        self.yLabel=yLabel
    def extractFileName(self, currfile):
        file = ntpath.basename(currfile)
        file = file.split(".")
        return (file[0])

    def calculateFileSize(self,fileContent):
        file_size=getsizeof(fileContent)
        return file_size


    def process_Files(self,sc,zippedRdd,asm_files,bytes_file):
        if self.yLabel==True :
            print('Here')
            asm_rdd = sc.wholeTextFiles(asm_files)
            bytes_rdd = sc.wholeTextFiles(bytes_file)
            asm_rdd = asm_rdd.map(lambda x: (self.extractFileName(x[0]), x[1])) # returns(hash,content)
            bytes_rdd = bytes_rdd.map(lambda x: (self.extractFileName(x[0]), x[1])) # returns(hash,content)
            bytes_rdd = bytes_rdd.map(lambda x: (x[0], self.calculateFileSize(x[1]))) # returns(hash,bytessize)
            asm_rdd = asm_rdd.map(lambda x: (x[0], self.calculateFileSize(x[1]))) # returns(hash,asmsize)

            zippedRdd = zippedRdd.map(lambda x: (x[0], (x[1][1], x[1][0])))
            resultant_rdd = bytes_rdd.join(asm_rdd)
            resultant_rdd = resultant_rdd.map(lambda x: (x[0], (x[1][0], x[1][1], float(x[1][1] / x[1][0]))))
            resultant_rdd = zippedRdd.join(resultant_rdd)
            resultant_rdd = resultant_rdd.map(
                lambda x: (x[1][0][0], x[0], x[1][1][0], x[1][1][1], x[1][1][2], x[1][0][1])) # returns(docid ,hash ,bytessize,asmsize,asm_bytes ratio,label)
            schema = StructType([StructField('docid', IntegerType(), True),
                                 StructField('hash', StringType(), True),
                                 StructField('bytes_size', IntegerType(), True),
                                 StructField('asm_size', IntegerType(), True),
                                 StructField('asm_byte_ratio', FloatType(), True),
                                 StructField('label', IntegerType(), True)
                                 ])
            resultant_DF = resultant_rdd.toDF(schema)
            if self.train==True :
                resultant_DF.write.save("gs://dsp-p2/trainFileFeatures/train-Map.parquet");
            else :
                resultant_DF.write.save("gs://dsp-p2/testFileFeatures/test-Map.parquet");

            resultant_DF.show()
            return resultant_DF;
        else :
            asm_rdd = sc.wholeTextFiles(asm_files)
            bytes_rdd = sc.wholeTextFiles(bytes_file)
            asm_rdd = asm_rdd.map(lambda x: (self.extractFileName(x[0]), x[1])) #returns(hash,filecontent)
            bytes_rdd = bytes_rdd.map(lambda x: (self.extractFileName(x[0]), x[1])) #returns(hash,filename)
            bytes_rdd = bytes_rdd.map(lambda x: (x[0], (self.calculateFileSize(x[1]))))
            asm_rdd = asm_rdd.map(lambda x: (x[0], (self.calculateFileSize(x[1]))))
            resultant_rdd=bytes_rdd.join(asm_rdd)
            resultant_rdd=resultant_rdd.map(lambda x:(x[0],(x[1][0],x[1][1],float(x[1][1]/x[1][0]))))
            zippedRdd=zippedRdd.map(lambda x:(x[0],(x[1])))
            resultant_rdd=zippedRdd.join(resultant_rdd)
            resultant_rdd=resultant_rdd.map(lambda x:(x[1][0],x[0],x[1][1][0],x[1][1][1],x[1][1][2])) # returns(docid ,hash ,bytessize,asmsize,asm_bytes ratio)
            schema = StructType([StructField('docid', IntegerType(), True),
                                 StructField('hash', StringType(), True),
                                 StructField('bytes_size', IntegerType(), True),
                                 StructField('asm_size', IntegerType(), True),
                                 StructField('asm_byte_ratio', FloatType(), True)

                                 ])
            resultant_DF = resultant_rdd.toDF(schema)

            resultant_DF.write.save("gs://dsp-p2/testFileFeatures/test-Map.parquet");
            resultant_DF.show()

            return resultant_DF




class File_Processor1(object) :
    '''
       This class takes in the bytes files and asm files size and the calculates the bytes size asm size and file size
       and ratio of bytes and asm size

       methods

        getFileDetails :- takes hash hits respective urls and then calculates (bytes ,asm and asm/bytes ratio)




       '''

    def __init__(self ,train=False,yLabel=False):
        self.train=train;
        self.yLabel=yLabel
    def getFileDetails(self,fileName):
        asmPath='https://storage.googleapis.com/uga-dsp/project2/data/asm/'+fileName+'.asm'
        binaryPath='https://storage.googleapis.com/uga-dsp/project2/data/bytes/'+fileName+'.bytes'
        site=req_file.urlopen(binaryPath)
        bytes_size=site.length;
        site = req_file.urlopen(asmPath)
        asm_size=site.length;
        asm_ratio=asm_size/bytes_size;
        return(bytes_size,asm_size,asm_ratio)
    def process_Files(self,sc,zippedRdd):


        if self.yLabel==True:

            file_stats_rdd = zippedRdd.map(lambda x: (x[1][1],x[0] ,self.getFileDetails(x[0]), x[1][0]))
            file_stats_rdd=file_stats_rdd.map(lambda x:(x[0],x[1],x[2][0],x[2][1],x[2][2],x[3]))
            schema = StructType([StructField('docid', IntegerType(), True),
                                 StructField('hash', StringType(), True),
                                 StructField('bytes_size', IntegerType(), True),
                                 StructField('asm_size', IntegerType(), True),
                                 StructField('asm_byte_ratio', FloatType(), True),
                                 StructField('label', IntegerType(), True)
                                 ])
            file_DF = file_stats_rdd.toDF(schema)
            if self.train==True:
                file_DF.write.save("gs://dsp-p2/trainFileFeatures/train-URL.parquet");
            else :
                file_DF.write.save("gs://dsp-p2/testFileFeatures/test-URL.parquet");
            print(file_DF.show())


        else:
            file_stats_rdd = zippedRdd.map(lambda x: (x[1],x[0], self.getFileDetails(x[0])))
            file_stats_rdd=file_stats_rdd.map(lambda x:(x[0],x[1],x[2][0],x[2][1],x[2][2]))


            schema = StructType([StructField('docid', IntegerType(), True),
                                 StructField('hash', StringType(), True),
                                 StructField('bytes_size', IntegerType(), True),
                                 StructField('asm_size', IntegerType(), True),
                                 StructField('asm_byte_ratio', FloatType(), True)
                                 ])
            file_DF=file_stats_rdd.toDF(schema)
            file_DF.write.save("./data/testFileFeatures/test-URL.parquet");
            file_DF.show()

        return file_DF
