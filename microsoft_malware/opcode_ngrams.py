# python packages
import argparse
import os.path
import re
import numpy as np
from operator import add

# pyspark packages
from pyspark import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.types import DoubleType
from pyspark.ml.feature import NGram
from pyspark.ml.linalg import Vectors
from pyspark.ml.feature import StringIndexer
from pyspark.ml.classification import RandomForestClassifier

def opcode_detect(asm_content):
    """
    Detects opcodes of the content of asm file.
    Returns a opcodes list.
    """
    pattern = re.compile(r'[\s][A-F0-9]{2}([\s]+)([a-z]+)[\s+]')
    pattern_list = pattern.findall(asm_content)
    return pattern_list

def opcode_ngram(df_opcode, N):
    """
    Generates n-grams opcode by opcode data frame.
    Returns n-grams opcode in RDD((filename, n-gram), total_counts)
    """
    ngrams = NGram(n = N, inputCol = "opcode", outputCol = "ngrams")
    df_ngrams = ngrams.transform(df_opcode)
    rdd_ngrams = df_ngrams.select("filename", "ngrams").rdd.map(tuple).flatMapValues(lambda x: x)\
                    .map(lambda x: ((x[0], x[1]), 1)).reduceByKey(add)
    return rdd_ngrams

def opcode_ngrams_combine(df_opcode, N):
    """
    Combines 2-grams, ..., N-grams opcode RDD
    >>> ((filename, opcode_ngrams), count)
    ordered by the count
    """
    rdd_ngrams_combine = opcode_ngram(df_opcode, 2)
    for n in range(2, N):
        rdd_ngrams = opcode_ngram(df_opcode, n+1)
        rdd_ngrams_combine = rdd_ngrams_combine.union(rdd_ngrams)
    return rdd_ngrams_combine

def feature_IDF(rdd_feature_detect):
    """
    Generates IDF values of each opcodes
    >>> Input ((hash, feature), cnt), Output (feature, IDF)
    """
    rdd_feature_IDF = rdd_feature_detect.distinct().map(lambda x: (x[1], x[0])).groupByKey()\
                        .map(lambda x: (x[0], len(list(x[1])))).filter(lambda x: x[1]<(n_train/3))\
                        .map(lambda x: (x[0], np.log(n_train/x[1]))).filter(lambda x: x[1]!=0)
    # rdd_feature_IDF = rdd_feature_detect.distinct().map(lambda x: (x[1], 1)).reduceByKey(add)\
    #                     .map(lambda x: (x[0], np.log(n_train/x[1])))#.filter(lambda x: x[1]!=0)
    return rdd_feature_IDF

def numpy_cartesian(feature_list, n):
    """
    Returns a numpy array containing 0s for not specified index,
    but feature counts for specified index.
    """
    feature_array = np.zeros((n,), dtype = np.int)
    if feature_list == None:
        feature_array = np.zeros((n,), dtype = np.int)
    else:
        f = np.asarray(feature_list)
        feature_array[f[:,0]] = f[:,1]
    return feature_array

def RF_features_select(rdd_feature_vd, n=10, m=7, s = 50):
    """
    Implements random forest classifier to the opcodes counts in each document
    Returns the importance of each opcodes
    >> Input (hash, label, features), Output (features, importance)
    """
    data_feature = rdd_feature_vd.map(lambda x: (x[1], x[2], x[3]))
    df = spark.createDataFrame(data_feature, ["hash", "label", "features"])

    stringIndexer = StringIndexer(inputCol = "hash", outputCol = "indexed")
    si_model = stringIndexer.fit(df)
    td = si_model.transform(df)
    rf = RandomForestClassifier(numTrees = n, maxDepth = m, labelCol = "label", seed = s)
    td_new = td.withColumn("label", td["label"].cast(DoubleType()))
    model = rf.fit(td_new)
    feature_imp = model.featureImportances
    return feature_imp

def feature_filter(rdd_feature_imp, rdd_feature_distinct, rdd_feature_cnt, rdd_train):
    """
    Filters out the trivial features (importance = 0)
    Returns the same format as (docid, hash, label, feature_list)
    """
    # >> (index, feature_imp)
    rdd_feature_imp = rdd_feature_imp.zipWithIndex().map(lambda x: (x[1], x[0]))
    # >> (index, (feature, feature_imp)) >> (feature, new_index)
    rdd_feature_choose = rdd_feature_distinct.map(lambda x: (x[1], x[0])).leftOuterJoin(rdd_feature_imp)\
                            .filter(lambda x: x[1][1]!=0).map(lambda x: x[1][0]).zipWithIndex()
    num = rdd_feature_choose.count()
    # >> (docid, hash, label, vector.dense(opcode))
    rdd = rdd_feature_cnt.map(lambda x: (x[0][1], (x[0][0], x[1])))\
                .leftOuterJoin(rdd_feature_choose).filter(lambda x: x[1][1]!=None)\
                .map(lambda x: (x[1][0][0], (x[1][1], x[1][0][1])))\
                .groupByKey().map(lambda x: (x[0], list(x[1])))
    feature = rdd_train.map(lambda x: (x[1], (x[0], x[2]))).leftOuterJoin(rdd)\
                .map(lambda x: (x[1][0][0], x[0], x[1][0][1], list(numpy_cartesian(x[1][1], N))))\
                .map(lambda x: (x[0], x[1], x[2], Vectors.dense(x[3])))
    return feature, rdd_feature_choose, num


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description = "CSCI 8360 Project 2",
        epilog = "answer key", add_help = "How to use",
        prog = "python opcode_ngrams.py [txt_files_path] [asm_files_path] [output_path]")

    # Required args
    parser.add_argument("file-path", help = "Directory of .txt files")
    parser.add_argument("asm-path", help = "Directory of .asm files")
    parser.add_argument("output-path", help = "Directory of outputs")
    # Optional args
    parser.add_argument("-s", "--size", choices = ["small", "large"], default = "small",
        help = "Size to the selected file: \"small\", \"large\" [Default: \"small\"]")

    args = vars(parser.parse_args())
    sc = SparkContext()
    spark = SparkSession.builder.master("local").appName("Word Count")\
                        .config("spark.some.config.option", "some-value").getOrCreate()

    file_path = str(args['file-path'])
    asm_path = str(args['asm-path'])
    output_path = str(args['output-path'])
    if args['size'] == "small": size = str('_small')
    if args['size'] == "large": size = str('')

    # .txt files
    print('***** Reading txt files *******************************************************************')
    rdd_Xtrain = sc.textFile(file_path + 'X'+size+'_train.txt').zipWithIndex().map(lambda x: (x[1], x[0]))
    rdd_ytrain = sc.textFile(file_path + 'y'+size+'_train.txt').zipWithIndex().map(lambda x: (x[1], x[0]))
    rdd_train = rdd_Xtrain.join(rdd_ytrain).sortByKey().map(lambda x: (x[0],) + x[1])
    rdd_Xtest = sc.textFile(file_path + 'X'+size+'_test.txt').zipWithIndex().map(lambda x: (x[1], x[0]))

    n_train = rdd_Xtrain.count()
    n_test = rdd_Xtest.count()

    # .asm files
    print('***** Reading asm training files **********************************************************')
    files_train = rdd_Xtrain.map(lambda x: asm_path+x[1]+'.asm').reduce(lambda accum,x: accum+','+x)
    rdd_asm_train = sc.wholeTextFiles(files_train)\
                    .map(lambda x: (os.path.basename(x[0]), x[1])).map(lambda x: (x[0].replace('.asm', ''), x[1]))
    print('***** Reading asm testing files ***********************************************************')
    files_test = rdd_Xtest.map(lambda x: asm_path+x[1]+'.asm').reduce(lambda accum,x: accum+','+x)
    rdd_asm_test = sc.wholeTextFiles(files_test)\
                    .map(lambda x: (os.path.basename(x[0]), x[1])).map(lambda x: (x[0].replace('.asm', ''), x[1]))

    # Training set
    # -------------------------------------------------------------------------
    print('***** Training set starts *****************************************************************')
    print('***** Detecting opcodes *******************************************************************')
    # >> (hash, opcode) _not distinct
    rdd_opcode_detect = rdd_asm_train.map(lambda x: (x[0], opcode_detect(x[1])))\
                            .flatMapValues(lambda x: x).map(lambda x: (x[0], x[1][1]))

    print('***** Creating opcodes list by IDF values *************************************************')
    # >> (opcode) _distinct
    opcode_list = feature_IDF(rdd_opcode_detect).map(lambda x: x[0]).collect()

    print('***** Filtering out opcodes with low IDF values *******************************************')
    # >> (filename, [opcodes_list])
    rdd_opcode_list = rdd_opcode_detect.filter(lambda x: x[1] in opcode_list)\
                            .groupByKey().map(lambda x: (x[0], list(x[1])))

    print('***** Generating n-gram opcodes ***********************************************************')
    df_opcode = spark.createDataFrame(rdd_opcode_list).toDF("filename", "opcode")
    # >> ((filename, opcode_ngrams), count)
    # rdd_opcode_cnt = opcode_ngrams_combine(df_opcode, 4)
    rdd_opcode_cnt = opcode_ngram(df_opcode, 4)

    print('***** Creating distinct n-grams opcodes list **********************************************')
    # >> (opcode, index)
    rdd_opcode_distinct = rdd_opcode_cnt.map(lambda x: x[0][1]).distinct()\
                            .sortBy(lambda x: x).zipWithIndex()
    N = rdd_opcode_distinct.count()

    print('***** Creating opcodes list for each document *********************************************')
    # >> (opcode, ((docid, hash, label), cnt))
    rdd_opcode = rdd_opcode_cnt.map(lambda x: (x[0][1], (x[0][0], x[1])))\
                        .leftOuterJoin(rdd_opcode_distinct)\
                        .map(lambda x: (x[1][0][0], (x[1][1], x[1][0][1])))\
                        .groupByKey().map(lambda x: (x[0], list(x[1])))

    print('***** Creating opcodes list with document information *************************************')
    # >> (docid, hash, label, vector.dense(opcode))
    opcode = rdd_train.map(lambda x: (x[1], (x[0], x[2]))).leftOuterJoin(rdd_opcode)\
                    .map(lambda x: (x[1][0][0], x[0], x[1][0][1], list(numpy_cartesian(x[1][1], N))))\
                    .map(lambda x: (x[0], x[1], x[2], Vectors.dense(x[3])))

    print('***** RF feature selection ****************************************************************')
    opcode_imp = RF_features_select(opcode)
    # >> (index, feature_importance)
    rdd_opcode_imp = sc.parallelize(opcode_imp)
    # opcode_r >> (docid, hash, label, vectors.dense(opcode))
    # rdd_opcode_distinct_r >> (opcode, index_r)
    opcode_r, rdd_opcode_distinct_r, N_r = feature_filter(rdd_opcode_imp, rdd_opcode_distinct, rdd_opcode_cnt, rdd_train)

    print('***** Transforming RDD into Dateframe *****************************************************')
    df_opcode_train_r = spark.createDataFrame(opcode_r)
    print('***** Outputing parquet file **************************************************************')
    df_opcode_train_r.write.parquet(output_path + "opcode" + size + "_train/")


    # Testing set
    # -------------------------------------------------------------------------
    print('***** Testing set starts ******************************************************************')
    print('***** Detecting segments ******************************************************************')
    # >> ((hash, segment), count)
    rdd_opcode_detect_test = rdd_asm_test.map(lambda x: (x[0], opcode_detect(x[1])))\
                                    .flatMapValues(lambda x: x).map(lambda x: (x[0], x[1][1]))

    print('***** Creating opcodes list for each document *********************************************')
    # >> (hash, [opcodes_list])
    rdd_opcode_list_test = rdd_opcode_detect_test.filter(lambda x: x[1] in opcode_list)\
                            .groupByKey().map(lambda x: (x[0], list(x[1])))

    print('***** Generating n-gram opcodes ***********************************************************')
    df_opcode_test = spark.createDataFrame(rdd_opcode_list_test).toDF("filename", "opcode")
    # >> ((hash, opcode), count)
    # rdd_opcode_cnt_test = opcode_ngrams_combine(df_opcode_test, 4)
    rdd_opcode_cnt_test = opcode_ngram(df_opcode, 4)

    print('***** Creating opcodes list for each document *********************************************')
    # >> (hash, (opcode_index, opcode_cnt))
    rdd_opcode_test = rdd_opcode_cnt_test.map(lambda x: (x[0][1], (x[0][0], x[1])))\
                            .leftOuterJoin(rdd_opcode_distinct_r).filter(lambda x: x[1][1]!=None)\
                            .map(lambda x: (x[1][0][0], (x[1][1], x[1][0][1])))\
                            .groupByKey().map(lambda x: (x[0], list(x[1])))

    print('***** Creating opcodes list with document information *************************************')
    # >> (docid, hash, vector.dense(opcode))
    opcode_test = rdd_Xtest.map(lambda x: (x[1], x[0]))\
                        .leftOuterJoin(rdd_opcode_test)\
                        .map(lambda x: (x[1][0], x[0], list(numpy_cartesian(x[1][1], N_r))))\
                        .map(lambda x: (x[0], x[1], Vectors.dense(x[2])))

    print('***** Transforming RDD into Dateframe *****************************************************')
    df_opcode_test = spark.createDataFrame(opcode_test)
    print('***** Outputing parquet file **************************************************************')
    df_opcode_test.write.parquet(output_path + "opcode" + size + "_test/")
