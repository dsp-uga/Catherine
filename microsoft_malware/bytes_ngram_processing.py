import urllib.request as ureq
import requests
import pyspark
import nltk
import numpy as np;
import os
import random

from pyspark.sql import SQLContext, SparkSession
from pyspark.ml.feature import NGram
from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import StringIndexer
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, NumericType, FloatType
import bottleneck

import math
from pyspark.ml.feature import NGram
import ntpath


class Bytes_Ngrams_processor(object):
    def __init__(self, ngrams=2, train=False, yLabel=False):
        if train == True:
            self.train = True;
        else:

            self.train = False;
        self.yLabel = yLabel;
        self.ngrams = ngrams

    def process(self, filePath):
        file = ntpath.basename(filePath)
        file = file.split(".")
        return (file[0])

    def cleanFile(self, data):
        rows = data.replace('\r\n', " ")
        rows = rows.split(" ")
        newRow = []
        for row in rows:
            if len(row) == 2 and row != '??':
                newRow.append(row)

        return newRow;

    def convertHexToNp(self, nparr):
        if self.ngrams < 2:
            byte_int = np.zeros(256);
            for row in nparr:
                try:
                    int_val = int(row, 16)
                except:
                    int_val = random.randint(0, 254);

                byte_int[int_val] += 1
        else:

            arrsize = math.pow(256, self.ngrams)
            byte_int = np.zeros(int(arrsize));
            for row in nparr:
                row = row.replace(" ", "")
                try:
                    int_val = int(row, 16);
                except:
                    int_val = random.randint(0, 254);

                byte_int[int_val] += 1
            byte_int = -bottleneck.partition(-byte_int, 1000)[:1000]

        return byte_int

    def computengrams(self, rows):

        if self.ngrams == 1:
            byte_int = self.convertHexToNp(rows)
        else:
            newRow = []
            for row in range(len(rows) - 1):
                currrow = rows[row] + rows[row + 1]
                newRow.append(currrow)
            byte_int = self.convertHexToNp(newRow)
        return byte_int;

    def extractNgramFeatures(self, sc, zipped_Rdd):
        if self.train == True:
            path = './bytes/train-1'
        else:
            path = './bytes/test'

        text_files_rdd = sc.wholeTextFiles(path);
        # text_files_rdd=text_files_rdd.zipWithIndex();
        ngrams_rdd = text_files_rdd.map(lambda x: (self.process(x[0]), x[1]))
        # try to calculate file size before this line
        ngrams_rdd_modified = ngrams_rdd.map(lambda x: (x[0], self.cleanFile(x[1])))


        ngramRdd = ngrams_rdd_modified.map(lambda x: (x[0], self.computengrams(x[1])))

        # SparseVector(256,np.nonzero(x[2])[0],x[2][x[2]>0])
        if self.ngrams == 1:
            arrLen = 256
        else:
            arrLen = 1000;
        if self.yLabel == True:
            resultantRdd = zipped_Rdd.join(ngramRdd)
            resultantRdd = resultantRdd.map(lambda x: (x[1][0][1], x[0], x[1][1], x[1][0][0]))
            resultantRdd = resultantRdd.map(
                lambda x: (x[0], x[1], SparseVector(arrLen, np.nonzero(x[2])[0], x[2][x[2] > 0]), x[3]))

            schema = StructType([StructField('docid', IntegerType(), True),
                                 StructField('hash', StringType(), True),
                                 StructField(str(self.ngrams) + 'grams', VectorUDT(), True),
                                 StructField('label', IntegerType(), True)
                                 ])
        else:
            resultantRdd = zipped_Rdd.join(ngramRdd)

            resultantRdd = resultantRdd.map(lambda x: (x[1][0], x[0], x[1][1]))
            resultantRdd = resultantRdd.map(
                lambda x: (x[0], x[1], SparseVector(arrLen, np.nonzero(x[2])[0], x[2][x[2] > 0])))
            # dummy=resultantRdd.foreach(lambda x:print(x))

            schema = StructType([StructField('docid', IntegerType(), True),
                                 StructField('hash', StringType(), True),
                                 StructField(str(self.ngrams) + 'grams', VectorUDT(), True)])

        resultantDF = resultantRdd.toDF(schema)

        if self.train == True:
            resultantDF.write.save("gs://dsp-p2/traingrams" + str(self.ngrams) + "gram.parquet");
        else:
            resultantDF.write.save("./data/test-bytes/test-" + str(self.ngrams) + "gram.parquet");

        return resultantDF;


'''
if __name__=='__main__' :
    spark = SparkSession \
        .builder \
        .appName("Catherine") \
        .getOrCreate()

    sc = spark.sparkContext
    X_train_file = sc.textFile('./files/X_train_small.txt')
    Y_train_file = sc.textFile('./files/Y_train_small.txt')
    X_train_file = X_train_file.zipWithIndex();
    Y_train_file = Y_train_file.zipWithIndex()
    X_train_file = X_train_file.map(lambda x: (x[1], x[0]))
    Y_train_file = Y_train_file.map(lambda x: (x[1], x[0]))

    zipped_bytes_rdd = X_train_file.join(Y_train_file)
    zipped_bytes_rdd = zipped_bytes_rdd.map(lambda x: (x[1][0], (int(x[1][1]),x[0])))
    byte_ngram_processor=Bytes_Ngrams_processor(2,True,True);
    print('train stage')
    train_DF = byte_ngram_processor.extractNgramFeatures(sc, zipped_bytes_rdd)
    print('done')
    print(train_DF.show())
    train_DF.write.save("./data/train-bytes/train-bigram.parquet")


    print('test')
    X_test_file = sc.textFile('./files/X_test_small.txt')
    Y_test_file = sc.textFile('./files/Y_test_small.txt')
    X_test_file = X_test_file.zipWithIndex();
    Y_test_file = Y_test_file.zipWithIndex()
    X_test_file = X_test_file.map(lambda x: (x[1], x[0]))
    Y_test_file = Y_test_file.map(lambda x: (x[1], x[0]))

    test_zipped_bytes_rdd = X_test_file.join(Y_test_file)
    test_zipped_bytes_rdd = test_zipped_bytes_rdd.map(lambda x: (x[1][0], (int(x[1][1]), x[0])))
    test_byte_ngram_processor = Bytes_Ngrams_processor(2,False,True);
    print('test stage')
    train_DF.write.save("./data/test-bytes/test-bigram.parquet")


    test_DF = test_byte_ngram_processor.extractNgramFeatures(sc, test_zipped_bytes_rdd)
    print(test_DF.show())

    rf = RandomForestClassifier(numTrees=200, impurity='gini', maxDepth=8, maxBins=32, seed=42)
    model = rf.fit(train_DF)
    print("model fit!!")

    # Make predictions.
    result = model.transform(test_DF)
    print("model transformed!!")
    # save to csv

    # get accuracy
    predictionAndLabels = result.select("prediction", "label")
    print(predictionAndLabels.show())
    evaluator = MulticlassClassificationEvaluator(metricName="accuracy")
    print("Accuracy: " + str(evaluator.evaluate(predictionAndLabels)))
    result.select('docid', 'hash', 'prediction').toPandas().to_csv('./results/finalLargePredictions.csv', header=False,
                                                                   index=False)
    print("results written to csv")
    spark.stop()

'''





