
import numpy as np;
import random

from pyspark.sql import SQLContext, SparkSession
from pyspark.ml.linalg import SparseVector, VectorUDT, Vectors

from pyspark.sql.types import StructType, StructField, StringType, IntegerType, NumericType, FloatType
from pyspark.ml.feature import CountVectorizer

import bottleneck

import math
from pyspark.ml.feature import NGram
import ntpath


class Bytes_Ngrams_processor(object):
    '''
    This class takes in the rdd and calculates ngrams depending on value of n
    Idea for 1 and 2 grams is that since hexadecimal digits forms 0-16 numbers in decimal system
    then we can create an array with 256 pow n as that many slots can be taken

    Methods

    extractNgramFeatures :--
      Takes in rdd and for n=1 and 2 grams this will calculate N-grams manually while for N>3 will calculate
      ngrams using Ngrams library

      returns a data frame containing docid hash ngram label

    '''
    def __init__(self, ngrams=1, train=False, yLabel=False):
        if train == True:
            self.train = True;
        else:

            self.train = False;
        self.yLabel = yLabel;
        self.ngrams = ngrams

    def process(self, filePath):
        file = ntpath.basename(filePath)
        file = file.split(".")
        return (file[0])

    def cleanFile(self, data):
        rows = data.replace('\r\n', " ")
        rows = rows.split(" ")
        newRow = []
        for row in rows:
            if len(row) == 2 and row != '??':
                newRow.append(row)

        return newRow;

    def convertToNumber(self, nparr):
        if self.ngrams < 2:
            byte_int = np.zeros(256);
            for row in nparr:
                try:
                    int_val = int(row, 16)
                except:
                    int_val = random.randint(0, 254);

                byte_int[int_val] += 1
        else:

            arrsize = math.pow(256, self.ngrams)
            byte_int = np.zeros(int(arrsize));
            for row in nparr:
                row = row.replace(" ", "")
                try:
                    int_val = int(row, 16);
                except:
                    int_val = random.randint(0, 254);

                byte_int[int_val] += 1
            byte_int = -bottleneck.partition(-byte_int, 2000)[:2000]

        return byte_int

    def computengrams(self, rows):

        if self.ngrams == 1:
            byte_int = self.convertToNumber(rows)
        else:
            newRow = []
            for row in range(len(rows) - 1):
                currrow = rows[row] + rows[row + 1]
                newRow.append(currrow)
            byte_int = self.convertToNumber(newRow)
        return byte_int;

    def extractNgramFeatures(self, sc, zipped_Rdd,path):
        if self.ngrams == 1:
            arrLen = 256
        else:
            arrLen = 2000;


        if self.yLabel==True :
            if self.ngrams <3 :
                text_files_rdd = sc.wholeTextFiles(path);
                # text_files_rdd=text_files_rdd.zipWithIndex();
                ngrams_rdd = text_files_rdd.map(lambda x: (self.process(x[0]), x[1]))
                # try to calculate file size before this line
                ngrams_rdd_modified = ngrams_rdd.map(lambda x: (x[0], self.cleanFile(x[1])))

                ngramRdd = ngrams_rdd_modified.map(lambda x: (x[0], self.computengrams(x[1])))
                resultantRdd = zipped_Rdd.join(ngramRdd)
                resultantRdd = resultantRdd.map(lambda x: (x[1][0][1], x[0], x[1][1], x[1][0][0]))
                resultantRdd = resultantRdd.map(
                    lambda x: (x[0], x[1], SparseVector(arrLen, np.nonzero(x[2])[0], x[2][x[2] > 0]), x[3]))

                schema = StructType([StructField('docid', IntegerType(), True),
                                     StructField('hash', StringType(), True),
                                     StructField(str(self.ngrams) + 'grams', VectorUDT(), True),
                                     StructField('label', IntegerType(), True)
                                     ])
                resultantDF = resultantRdd.toDF(schema)

            else:
                text_files_rdd=sc.wholeTextFiles(path)
                ngrams_rdd = text_files_rdd.map(lambda x: (self.process(x[0]), x[1]))

                ngrams_rdd=ngrams_rdd.map(lambda x:(x[0],self.cleanFile(x[1])))
                sql=SQLContext(sc)
                ngrams_DF=sql.createDataFrame(ngrams_rdd,["hash","file"])

                ngram = NGram(n=self.ngrams, inputCol="file", outputCol="ngrams")

                ngramDataFrame = ngram.transform(ngrams_DF)
                ngramDataFrame=ngramDataFrame.drop('file')
                cv = CountVectorizer(inputCol="ngrams", outputCol=str(self.ngrams)+"grams", vocabSize=math.pow(256,self.ngrams), minDF=2.0)
                model = cv.fit(ngramDataFrame)

                ngramDataFrame = model.transform(ngramDataFrame)
                ngramDataFrame=ngramDataFrame.drop('ngrams')
                zipped_Rdd=zipped_Rdd.map(lambda x:(x[1][1],x[0],x[1][0]))
                zipped_DataFrame=sql.createDataFrame(zipped_Rdd,["docid","hash","label"])

                resultantDF=zipped_DataFrame.join(ngramDataFrame,["hash"])



                #dummy=ngrams_rdd.foreach(lambda x:print(x))



        else:

            if self.ngrams < 3:
                text_files_rdd = sc.wholeTextFiles(path);
                ngrams_rdd = text_files_rdd.map(lambda x: (self.process(x[0]), x[1]))
                # try to calculate file size before this line
                ngrams_rdd_modified = ngrams_rdd.map(lambda x: (x[0], self.cleanFile(x[1])))

                ngramRdd = ngrams_rdd_modified.map(lambda x: (x[0], self.computengrams(x[1])))

                resultantRdd = zipped_Rdd.join(ngramRdd)

                resultantRdd = resultantRdd.map(lambda x: (x[1][0], x[0], x[1][1]))
                resultantRdd = resultantRdd.map(
                    lambda x: (x[0], x[1], SparseVector(arrLen, np.nonzero(x[2])[0], x[2][x[2] > 0])))
                # dummy=resultantRdd.foreach(lambda x:print(x))

                schema = StructType([StructField('docid', IntegerType(), True),
                                     StructField('hash', StringType(), True),
                                     StructField(str(self.ngrams) + 'grams', VectorUDT(), True)])
                resultantDF = resultantRdd.toDF(schema)

            else:
                text_files_rdd = sc.wholeTextFiles(path)
                ngrams_rdd = text_files_rdd.map(lambda x: (self.process(x[0]), x[1]))

                ngrams_rdd = ngrams_rdd.map(lambda x: (x[0], self.cleanFile(x[1])))
                sql = SQLContext(sc)
                ngrams_DF = sql.createDataFrame(ngrams_rdd, ["hash", "file"])

                ngram = NGram(n=self.ngrams, inputCol="file", outputCol="ngrams")

                ngramDataFrame = ngram.transform(ngrams_DF)
                ngramDataFrame = ngramDataFrame.drop('file')
                cv = CountVectorizer(inputCol="ngrams", outputCol=str(self.ngrams) + "grams",
                                     vocabSize=math.pow(256, self.ngrams), minDF=2.0)
                model = cv.fit(ngramDataFrame)

                ngramDataFrame = model.transform(ngramDataFrame)
                ngramDataFrame = ngramDataFrame.drop('ngrams')
                zipped_DataFrame=sql.createDataFrame(zipped_Rdd,["hash","docid"])
                resultantDF = zipped_DataFrame.join(ngramDataFrame, ["hash"])




        if self.train == True:
            resultantDF.write.save("gs://dsp-p2/trainNgrams/train" + str(self.ngrams) + ".parquet");
        else:
            resultantDF.write.save("gs://dsp-p2/testNgrams/test" + str(self.ngrams) + ".parquet");

        return resultantDF;









